{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6871555c",
   "metadata": {},
   "source": [
    "SlothPy makes extensive use of the HDF5 file format (powerful and portable binary format for fast I/O and big data) to store all the data related to the software. The core file format .slt is in fact .hdf5/h5 file in disguise, so it can be additionally opened and modified with programs such as HDFView and HDFCompas programs or directly accessed through the h5py Python wrapper. Firstly, let us import the pacgake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eba57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import slothpy as slt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23899e9f",
   "metadata": {},
   "source": [
    "To start using SlothPy one has to create at least one instance of the core class object - Compound. The Compound is intrinsically associated with a .slt file stored on your disk. You can create the Compound from an output file of quantum chemistry software, access the existing .slt file, or add more data to it. Those operations are handled by the Compound creation methods. There is one .rassi.h5 file produced from the MOLCAS calculation included in the \"examples\" folder from our repository on GitHub. We will use it as an example in this tutorial. To create a Compound from it we have to include its relative path and use slt.compound_from_molcas() method. Note: always check the documentation of the used methods on our website or use your editor to do so (e.g. Shift+Tab in Jupyter Notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29267b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "NdCo = slt.compound_from_molcas(\".\", \"Nd_tutorial\", \"bas3\", \"./examples\", \"NdCo_DG_bas3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a5d4f",
   "metadata": {},
   "source": [
    "After the creation, you can check what is inside your file using the print() method or like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becceb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NdCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba935eb4",
   "metadata": {},
   "source": [
    "You can see the list of HDF5 groups and data sets contained in the file together with their Description attributes. This is the way that SlothPy will save all your results. Having already .slt file on your disk you can add to it more ab initio results (just use the path and name of an existing file) or access it at a later point using slt.compound_from_slt() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "NdCo = slt.compound_from_slt(\".\", \"Nd_tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971fbfa",
   "metadata": {},
   "source": [
    "All available methods are accessed through an instance of the Compound class that constitutes the user interface and API documented in the Reference Manual. Let us start by computing molar powder-averaged magnetisation for our Nd-based compound. Firstly, we need some imports to create a range of magnetic field and temperature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "\n",
    "fields_mth = linspace(0.0001, 7, 50)\n",
    "temperatures_mth = linspace(1, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f15aa",
   "metadata": {},
   "source": [
    "The method for computing magnetisation as a function of field and temperature is called calculate_mth()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04dd880",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_mth, 4, temperatures_mth, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbfd3dca",
   "metadata": {},
   "source": [
    "Here, we calculate powder-averaged (using Lebedev-Laikov integration grid) magnetisation for the Nd-based compound in the magnetic field range from 1 Oe to 7 T (50 points) and for 10 temperature values from 1 K to 10 K. We include in the Zeeman Hamiltonian only 10 Spin-Orbit states from the ground multiplet $ ^{4}I_{9/2} $ of Nd(III). The calculation should finish immediately due to the very low amount of SO states included.\n",
    "\n",
    "The result is a numpy NdArray (10, 50) with the structure [temperatures, fields] returned from the function call to the mth variable. It is ready for you to process it using Python as you want.  Remember to always check the output format in the Reference Manual (or using your editor/IDE) - Returns section of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6b2c8",
   "metadata": {},
   "source": [
    "We can, for example, plot it as a magnetic field function M(H) iterating over different temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import plot, show\n",
    "\n",
    "for mh in mth:\n",
    "    plot(fields_mth, mh)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51eb2f",
   "metadata": {},
   "source": [
    "Now, let us run the above calculation once again, but this time we will use a better, denser grid and include more SO-states. It will take a little more time. Additionally, we will save the results to our Nd_tutorial.slt file using slt keyword. (confront the documentation for the comprehensive description of all the options) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fd1a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_mth, 6, temperatures_mth, 32, slt=\"bas3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad1c5c",
   "metadata": {},
   "source": [
    "If you invoke the representation of the Nd_tutorial.slt file once again you can see that a new group \"bas3_magnetisation\" was created which contains datasets for magnetisation (bas3_mth), magnetic fields (bas3_fields), and temperatures (bas3_temperatures). (all with Description attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2424f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NdCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e9dda",
   "metadata": {},
   "source": [
    "SlothPy provides an array-like interface for reading and writing data from and to .slt files. As an example, let us read the magnetisation to another variable mth_custom_read together with field values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abdd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "mth_custom_read = NdCo[\"bas3_magnetisation\", \"bas3_mth\"]\n",
    "field_values = NdCo[\"bas3_magnetisation\", \"bas3_fields\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70828263",
   "metadata": {},
   "source": [
    "Here, we provide a full group and dataset name to access the data. Now we can do what we want with the arrays. Let us confirm that they indeed represent magnetisation once again by plotting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ac510",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mh in mth_custom_read:\n",
    "    plot(field_values, mh)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72964863",
   "metadata": {},
   "source": [
    "Since we have our data saved in the .slt file we can actually plot it using the build-in function (available for various methods - see documentation, all starting with \"plot\" and having plenty of customization parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25833511",
   "metadata": {},
   "outputs": [],
   "source": [
    "NdCo.plot_mth(\"bas3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9ab8f",
   "metadata": {},
   "source": [
    "When you invoke plotting functions for specific methods you do not need to provide a suffix, like \"_magnetisation\", the program will handle it for you.\n",
    "\n",
    "If you need you can even create your own custom groups with datasets (in the form of numpy NdArrays) in the file and use them in your scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e3b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_to_ten = linspace(1, 10, 10)\n",
    "NdCo[\"my_custom_group\", \"one_to_ten_dataset\", \"My description of the dataset\",\n",
    "     \"My description of the whole group\"] = one_to_ten\n",
    "NdCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac773c3b",
   "metadata": {},
   "source": [
    "The last two strings giving a description of the group and data set are optional. Later you can add to the existing group more data sets or create datasets without a group (they have to be at least 1-dimensional ArrayLike):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f144e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NdCo[\"my_custom_group\", \"123_dataset\"] = [1,2,3]\n",
    "NdCo[\"my_dataset_without_a_group\"] = [1]\n",
    "NdCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05760249",
   "metadata": {},
   "source": [
    "If you now try to re-run the previous calculation you should see a SlothPyError, due to the already existing name of the group (SlothPy prevents you from accidentally overwriting the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f700e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_mth, 6, temperatures_mth, 32, slt=\"bas3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9a192",
   "metadata": {},
   "source": [
    "We can use delete_group_dataset to manually remove datases/groups from the .slt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b940f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NdCo.delete_group_dataset(\"my_dataset_without_a_group\")\n",
    "NdCo.delete_group_dataset(\"my_custom_group\", \"123_dataset\")\n",
    "NdCo.delete_group_dataset(\"bas3_magnetisation\")\n",
    "NdCo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc3c89",
   "metadata": {},
   "source": [
    "As you already should notice (when reading the docstring of calculate_mth method) SlopthPy provides you full control over the amount of CPUs you want to assign to your calculation and threads to be used. Computationally demanding methods are parallelized using a certain amount of separate processes - the number of processes that will be used is (number of CPUs) // (number of threads). Additionally, in the Note section of each method, the user is informed over what quantity the job will be parallelized. In the case of calcualte_mth, the work is distributed over field values (here 50). So using for example 10 parallel processes each will compute the magnetisation for 5 values of the field. As default, SlothPy uses all available logical cores with 1 thread for the linear algebra libraries. For jobs with a very high number of points to be parallelized, you should benefit from a greater number of processes (not including time for Python's multiprocessing setup for a huge amount of data). On the other hand with increasing matrices size it is beneficial to use more threads for operations such as diagonalization etc. It is not a trivial task to choose good settings for very demanding calculations that is why we provide, within SlothPy, the autotune module to do it for you. It tests all possible meaningful setups and gives you time estimates for each of them. It takes some time to do this (because it actually truly does a part of the calculations to benchmark them) so it is advised to use it for jobs that will take hours. To demonstrate it with very small matrices provided in our file (they are 364 x 364 - that is how many SO states are there) we will run two examples using all available CPUs on your machine (if you want to leave some you should change number_cpu = 0 to your desired number):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fe6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_process = linspace(0.0001, 7, 60)\n",
    "fields_threads = linspace(0.0001, 7, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4708d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_process, 6, temperatures_mth, 364, number_cpu = 0, autotune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c97efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_threads, 6, temperatures_mth, 364, number_cpu = 0, autotune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899306c5",
   "metadata": {},
   "source": [
    "In the first case autotune module should choose (but depending on your hardware) more processes and fewer threads than in the second one, where we paralleize only over 3 field points. Time estimates include only pure calculation steps and in our tests for a variety of different methods they should give results within 15-20% of overall execution time maximal error. After autotuning you can run the calculation with the chosen setting manually to see how much time it will take compared to our estimate. Can you choose better settings by yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cpu = #fill here the number you were autotuning for\n",
    "num_of_threads = #fill here the number of threads chosen by the autotune module (for fields_process and _threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_process, 6, temperatures_mth, 364, num_of_cpu, num_of_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mth = NdCo.calculate_mth(\"bas3\", fields_threads, 6, temperatures_mth, 364, num_of_cpu, num_of_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399da4a1",
   "metadata": {},
   "source": [
    "The necessity of the autotune module will become visible for matrices with a number of states over 500-1000 or even 2000+ when calculations with certain settings (how many field values and grids) can take many hours or even days. It also all depends on your hardware e.g. how many possibilities is there to check. For me writing this tutorial now I am testing it on 128 logical cores (64 physical) CPU, so there are many possibilities to choose a number of threads and processes - therefore it is harder and also more time-consuming for the module, but still better than trying manually all the possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90bb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
